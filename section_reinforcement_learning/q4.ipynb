{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../scripts/')\n",
    "from dp_policy_agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateInfo: ###q4stateinfo\n",
    "    def __init__(self, action_num, epsilon=0.3):\n",
    "        self.q = np.zeros(action_num)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def greedy(self):\n",
    "        return np.argmax(self.q)\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon): \n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(range(len(self.q)))\n",
    "        else:\n",
    "            return self.greedy()\n",
    "    \n",
    "    def pi(self):\n",
    "        return self.epsilon_greedy(self.epsilon) \n",
    "    \n",
    "    def max_q(self):              #追加\n",
    "        return max(self.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(DpPolicyAgent): ###q4qagent\n",
    "    def __init__(self, time_interval, estimator, puddle_coef=100,  alpha=0.5, widths=np.array([0.2, 0.2, math.pi/18]).T, \\\n",
    "                 lowerleft=np.array([-4, -4]).T, upperright=np.array([4, 4]).T, dev_borders=[0.1,0.2,0.4,0.8]): #alpha追加\n",
    "        super().__init__(time_interval, estimator, None, puddle_coef, widths, lowerleft, upperright)\n",
    "        \n",
    "        nx, ny, nt = self.index_nums\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt)))\n",
    "        self.actions = list(set([tuple(self.policy_data[i]) for i in self.indexes]))\n",
    "        self.ss = self.set_action_value_function()\n",
    "        \n",
    "        ###強化学習用変数### #追加\n",
    "        self.alpha = alpha\n",
    "        self.s, self.a = None, None\n",
    "        self.update_end = False\n",
    "\n",
    "    def set_action_value_function(self): #状態価値関数を読み込んで行動価値関数を初期化\n",
    "        ss = {} #state spaceという意味\n",
    "        for line in open(\"puddle_ignore_values.txt\", \"r\"): #価値のファイルを読み込む\n",
    "            d = line.split()\n",
    "            index, value = (int(d[0]), int(d[1]), int(d[2])), float(d[3]) #インデックスをタプル、値を数字に\n",
    "            ss[index] = StateInfo(len(self.actions)) #StateInfoオブジェクトを割り当てて初期化\n",
    "            \n",
    "            for i, a in enumerate(self.actions): #方策の行動価値を価値のファイルに書いてある値に。そうでない場合はちょっと引く\n",
    "                ss[index].q[i] = value if tuple(self.policy_data[index]) == a else value - 0.1\n",
    "                \n",
    "        return ss\n",
    "    \n",
    "    def policy(self, pose, goal=None): #q4policy\n",
    "        index = self.to_index(pose, self.pose_min, self.index_nums, self.widths)               \n",
    "        s = tuple(index)\n",
    "        a = self.ss[s].pi()\n",
    "        return s, a                 #状態をタプルにしたものと、行動のインデックスで返すように変更\n",
    "    \n",
    "    def decision(self, observation=None):###q4decision\n",
    "        ###終了処理###\n",
    "        if self.update_end:  return 0.0, 0.0\n",
    "        if self.in_goal:          self.update_end = True #ゴールに入った後も一回だけ更新があるので即終了はしない\n",
    "        \n",
    "        ###カルマンフィルタの実行###\n",
    "        self.estimator.motion_update(self.prev_nu, self.prev_omega, self.time_interval)\n",
    "        self.estimator.observation_update(observation)\n",
    "        \n",
    "        ###行動決定と報酬の処理###\n",
    "        s_, a_ = self.policy(self.estimator.pose) #KFの結果から前回の状態遷移後の状態s'と次の行動a_（max_a'のa'ではないことに注意）を得る\n",
    "        r = self.time_interval*self.reward_per_sec() #状態遷移の報酬\n",
    "        self.total_reward += r\n",
    "        \n",
    "        ###Q学習と現在の状態と行動の保存###\n",
    "        self.q_update(self.s, self.a, r, s_)      #self.s, self.aがQ値更新対象の状態行動対。報酬rと次の状態s_を使って更新。\n",
    "        self.s, self.a = s_, a_\n",
    "\n",
    "        ###出力###\n",
    "        self.prev_nu, self.prev_omega = self.actions[a_]\n",
    "        return self.actions[a_]\n",
    "    \n",
    "    def q_update(self, s, a, r, s_):###q4update\n",
    "        if s == None:  return\n",
    "        \n",
    "        q = self.ss[s].q[a]\n",
    "        q_ = self.final_value if self.in_goal else self.ss[s_].max_q()\n",
    "        self.ss[s].q[a] = (1.0 - self.alpha)*q + self.alpha*(r + q_)\n",
    "\n",
    "        ###ログをとる（あとから削除）###\n",
    "        with open(\"log.txt\", \"a\") as f:\n",
    "            f.write(\"{} {} {} prev_q:{:.2f}, next_step_max_q:{:.2f}, new_q:{:.2f}\\n\".format(s, r, s_, q, q_, self.ss[s].q[a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(): \n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(400000, time_interval, debug=False)  #長時間アニメーション時間をとる\n",
    "\n",
    "    ## 地図を生成して3つランドマークを追加 ##\n",
    "    m = Map()\n",
    "    for ln in [(-4,2), (2,-3), (4,4), (-4,-4)]: m.append_landmark(Landmark(*ln))\n",
    "    world.append(m)   \n",
    "\n",
    "    ##ゴールの追加##\n",
    "    goal = Goal(-3,-3) \n",
    "    world.append(goal)\n",
    "    \n",
    "    ##水たまりの追加##\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1)) \n",
    "\n",
    "    ##ロボットを1台登場させる##\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    kf = KalmanFilter(m, init_pose)\n",
    "    a = QAgent(time_interval, kf) #goalを削除\n",
    "    r = Robot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "    world.append(r)\n",
    "    \n",
    "    world.draw()\n",
    "    return a\n",
    "    \n",
    "a = trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
