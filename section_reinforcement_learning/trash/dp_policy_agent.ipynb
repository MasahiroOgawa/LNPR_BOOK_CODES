{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ###dppolicyagent\n",
    "sys.path.append('../scripts/')\n",
    "from puddle_world import *\n",
    "import itertools\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DpPolicyAgent(KfAgent):  ###dppolicyagent\n",
    "    def __init__(self, time_interval, init_pose, envmap, \\\n",
    "                motion_noise_stds={\"nn\":0.19, \"no\":0.001, \"on\":0.13, \"oo\":0.2}, widths=np.array([0.2, 0.2, math.pi/18]).T, \\\n",
    "                 lowerleft=np.array([-4, -4]).T, upperright=np.array([4, 4]).T): #widths以降はDynamicProgrammingから持ってくる\n",
    "        super().__init__(time_interval, 0.0, 0.0, init_pose, envmap, motion_noise_stds) \n",
    "        \n",
    "        ###座標関連の変数をDynamicProgrammingから持ってくる###\n",
    "        self.pose_min = np.r_[lowerleft, 0] \n",
    "        self.pose_max = np.r_[upperright, math.pi*2]\n",
    "        self.widths = widths\n",
    "        self.index_nums = ((self.pose_max - self.pose_min)/self.widths).astype(int)\n",
    "        \n",
    "        self.policy_data = self.init_policy()\n",
    "        \n",
    "    def init_policy(self):\n",
    "        tmp = np.zeros(np.r_[self.index_nums,2])\n",
    "        for line in open(\"/Users/ueda/GIT/LoPR_BOOK/reinforcement_learning_results/sarsa0_policy3500000.txt\", \"r\"):\n",
    "            d = line.split()\n",
    "            tmp[int(d[0]), int(d[1]), int(d[2])] = [float(d[3]), float(d[4])]\n",
    "            \n",
    "        return tmp\n",
    "        \n",
    "    def policy(self, pose): #姿勢から離散状態のインデックスを作って方策を参照して返すだけ\n",
    "        index = np.floor((pose - self.pose_min)/self.widths).astype(int)           #姿勢からインデックスに\n",
    "        \n",
    "        index[2] = (index[2] + self.index_nums[2]*1000)%self.index_nums[2] #角度の正規化\n",
    "        for i in [0,1]:                                                                                   #端の処理（内側の座標の方策を使う）\n",
    "            if index[i] < 0: index[i] = 0\n",
    "            elif index[i] >= self.index_nums[i]: index[i] = self.index_nums[i] - 1\n",
    "                \n",
    "        a = self.policy_data[tuple(index)]\n",
    "        return self.policy_data[tuple(index)] if random.random() > 0.3 else random.choice([[1, 0],[0, -2], [0, 2]]) \n",
    "        \n",
    "    def decision(self, observation=None):\n",
    "        self.kf.motion_update(self.prev_nu, self.prev_omega, self.time_interval)\n",
    "        self.kf.observation_update(observation)\n",
    "        \n",
    "        nu, omega = self.policy(self.kf.belief.mean)\n",
    "        self.prev_nu, self.prev_omega = nu, omega\n",
    "        return nu, omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  ###dppolicyagentrun\n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(300, time_interval) \n",
    "\n",
    "    m = Map()\n",
    "    m.append_landmark(Landmark(-4,2))\n",
    "    m.append_landmark(Landmark(2,-3))\n",
    "    m.append_landmark(Landmark(4,4))\n",
    "    m.append_landmark(Landmark(-4,-4))\n",
    "    world.append(m)\n",
    "    \n",
    "    ###ゴールの追加###\n",
    "    goal = Goal(-3,-3)\n",
    "    world.append(goal)\n",
    "    \n",
    "    ###水たまりの追加###\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1))\n",
    "\n",
    "    ### いくつかの初期位置を定義 ###   ###dppolicyagentrun\n",
    "    init_poses = []\n",
    "    for p in [[-1, 3, math.pi], [1, 3, math.pi], [3, 3, math.pi], [3, 1, math.pi], [3, -1, math.pi]]:\n",
    "        init_pose = np.array(p).T\n",
    "    \n",
    "        a = DpPolicyAgent(time_interval, init_pose, m)\n",
    "        r = Robot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "\n",
    "        world.append(r)\n",
    "        \n",
    "    world.draw()\n",
    "    #r.one_step(0.1) #デバッグ時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
