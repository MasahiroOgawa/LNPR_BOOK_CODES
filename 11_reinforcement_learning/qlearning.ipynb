{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa177bb",
   "metadata": {},
   "source": [
    "## 11.1.2 preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6369c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe48a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_policy_agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1390c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(DpPolicyAgent):\n",
    "    '''\n",
    "    Initial version\n",
    "    '''\n",
    "    def __init__(self, time_interval, estimator, goal, puddle_coeff=100, widths=np.array([0.2,0.2,math.pi/18]).T,\\\n",
    "                lowerleft=np.array([-4,-4]).T, upperright=np.array([4,4]).T):\n",
    "        super().__init__(time_interval, estimator, goal, puddle_coeff, widths, lowerleft, upperright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(): \n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(400000, time_interval, debug=False)  #長時間アニメーション時間をとる\n",
    "\n",
    "    ## 地図を生成して3つランドマークを追加 ##\n",
    "    m = Map()\n",
    "    for ln in [(-4,2), (2,-3), (4,4), (-4,-4)]: m.append_landmark(Landmark(*ln))\n",
    "    world.append(m)   \n",
    "\n",
    "    ##ゴールの追加##\n",
    "    goal = Goal(-3,-3) \n",
    "    world.append(goal)\n",
    "    \n",
    "    ##水たまりの追加##\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1)) \n",
    "\n",
    "    ##ロボットを1台登場させる##\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    kf = KalmanFilter(m, init_pose)\n",
    "    a = QAgent(time_interval, kf, goal)\n",
    "    r = Robot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "    world.append(r)\n",
    "    \n",
    "    world.draw()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cca252",
   "metadata": {},
   "source": [
    "## 11.1.3 set up Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateInfo:\n",
    "    '''\n",
    "    class for keeping Q values\n",
    "    '''\n",
    "    def __init__(self, action_num=3):\n",
    "        self.q = np.zeros(action_num)\n",
    "    \n",
    "    def greedy(self):\n",
    "        return np.argmax(self.q)\n",
    "    \n",
    "    def pi(self):\n",
    "        return self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(DpPolicyAgent):\n",
    "    '''\n",
    "    Second version\n",
    "    '''\n",
    "    def __init__(self, time_interval, estimator, puddle_coeff=100, widths=np.array([0.2,0.2,math.pi/18]).T,\\\n",
    "                lowerleft=np.array([-4,-4]).T, upperright=np.array([4,4]).T):\n",
    "        super().__init__(time_interval, estimator, None, puddle_coeff, widths, lowerleft, upperright)\n",
    "        \n",
    "        #const\n",
    "        self.smallval = 0.1 # initial value is subtracted with this value in case the action is not in policy.\n",
    "\n",
    "        nx, ny, nt = self.index_nums # in DpPolicyAgent. nx is number of x indices.\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt)))\n",
    "        self.actions = list(set([tuple(self.policy_data[i]) for i in self.indexes])) # in this example, actions=[forward,turn]\n",
    "        self.statespace = self.set_action_value_function() \n",
    "        \n",
    "    def set_action_value_function(self, value_file=\"../section_reinforcement_learning/puddle_ignore_values.txt\"):\n",
    "        statespace = {}\n",
    "        for line in open(value_file, 'r'):\n",
    "            d = line.split()\n",
    "            index= (int(d[0]), int(d[1]), int(d[2]))\n",
    "            value = float(d[3])\n",
    "            statespace[index] = StateInfo(len(self.actions)) # init state space\n",
    "            \n",
    "            for i, a in enumerate(self.actions):\n",
    "                statespace[index].q[i] = value if tuple(self.policy_data[index]) == a\\\n",
    "                else value - self.smallval\n",
    "            \n",
    "        return statespace\n",
    "    \n",
    "    def policy(self, pose, goal=None): # we need goal argument to overload!\n",
    "        index = self.to_index(pose, self.pose_min, self.index_nums, self.widths) # from DpPolicyAgent class\n",
    "        a = self.statespace[tuple(index)].pi() # a is the index of the action.\n",
    "        return self.actions[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(): \n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(400000, time_interval, debug=False)  #長時間アニメーション時間をとる\n",
    "\n",
    "    ## 地図を生成して3つランドマークを追加 ##\n",
    "    m = Map()\n",
    "    for ln in [(-4,2), (2,-3), (4,4), (-4,-4)]: m.append_landmark(Landmark(*ln))\n",
    "    world.append(m)   \n",
    "\n",
    "    ##ゴールの追加##\n",
    "    goal = Goal(-3,-3) \n",
    "    world.append(goal)\n",
    "    \n",
    "    ##水たまりの追加##\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1)) \n",
    "\n",
    "    ##ロボットを1台登場させる##\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    kf = KalmanFilter(m, init_pose)\n",
    "    a = QAgent(time_interval, kf)\n",
    "    r = Robot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "    world.append(r)\n",
    "    \n",
    "    world.draw()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0390a",
   "metadata": {},
   "source": [
    "## 11.1.4 espilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec749f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateInfo:\n",
    "    '''\n",
    "    class for keeping Q values\n",
    "    version2.\n",
    "    '''\n",
    "    def __init__(self, action_num=3, epsilon=0.7):\n",
    "        self.q = np.zeros(action_num)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def greedy(self):\n",
    "        return np.argmax(self.q)\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(range(len(self.q)))\n",
    "        else:\n",
    "            return self.greedy()\n",
    "    \n",
    "    def pi(self):\n",
    "        return self.epsilon_greedy(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a501a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e28ee",
   "metadata": {},
   "source": [
    "## 11.1.5 update Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateInfo:\n",
    "    '''\n",
    "    class for keeping Q values\n",
    "    version2.\n",
    "    '''\n",
    "    def __init__(self, action_num=3, epsilon=0.7):\n",
    "        self.q = np.zeros(action_num)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def greedy(self):\n",
    "        return np.argmax(self.q)\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(range(len(self.q)))\n",
    "        else:\n",
    "            return self.greedy()\n",
    "    \n",
    "    def pi(self):\n",
    "        return self.epsilon_greedy(self.epsilon)\n",
    "    \n",
    "    def max_q(self):\n",
    "        return max(self.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b315508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(DpPolicyAgent):\n",
    "    '''\n",
    "    Third version\n",
    "    '''\n",
    "    def __init__(self, time_interval, estimator, puddle_coeff=100, alpha=0.5,\\\n",
    "                 widths=np.array([0.2,0.2,math.pi/18]).T,\\\n",
    "                lowerleft=np.array([-4,-4]).T, upperright=np.array([4,4]).T):\n",
    "        super().__init__(time_interval, estimator, None, puddle_coeff, widths, lowerleft, upperright)\n",
    "        \n",
    "        #const\n",
    "        self.smallval = 0.1 # initial value is subtracted with this value in case the action is not in policy.\n",
    "\n",
    "        nx, ny, nt = self.index_nums # in DpPolicyAgent. nx is number of x indices.\n",
    "        self.indexes = list(itertools.product(range(nx), range(ny), range(nt)))\n",
    "        self.actions = list(set([tuple(self.policy_data[i]) for i in self.indexes])) # in this example, actions=[forward,turn]\n",
    "        self.statespace = self.set_action_value_function() \n",
    "        self.alpha = alpha # weight of new value for update Q \n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.update_end = False # whether Q update is finished or not.\n",
    "        \n",
    "        \n",
    "    def set_action_value_function(self, value_file=\"../section_reinforcement_learning/puddle_ignore_values.txt\"):\n",
    "        statespace = {}\n",
    "        for line in open(value_file, 'r'):\n",
    "            d = line.split()\n",
    "            index= (int(d[0]), int(d[1]), int(d[2]))\n",
    "            value = float(d[3])\n",
    "            statespace[index] = StateInfo(len(self.actions)) # init state space\n",
    "            \n",
    "            for i, a in enumerate(self.actions):\n",
    "                statespace[index].q[i] = value if tuple(self.policy_data[index]) == a\\\n",
    "                else value - self.smallval\n",
    "            \n",
    "        return statespace\n",
    "    \n",
    "    \n",
    "    def policy(self, pose, goal=None): # we need goal argument to overload!\n",
    "        index = self.to_index(pose, self.pose_min, self.index_nums, self.widths) # from DpPolicyAgent class\n",
    "        s = tuple(index)\n",
    "        a = self.statespace[s].pi() # a is the index of the action.\n",
    "        return s,a\n",
    "    \n",
    "    \n",
    "    def decision(self, observation=None):\n",
    "        if self.update_end: return 0.0, 0.0\n",
    "        if self.in_goal: self.update_end = True\n",
    "            \n",
    "        # observe self position by Kalman filter\n",
    "        self.estimator.motion_update(self.prev_nu, self.prev_omega, self.time_interval)\n",
    "        self.estimator.observation_update(observation)\n",
    "        \n",
    "        # deside action\n",
    "        next_s, next_a = self.policy(self.estimator.pose)\n",
    "        r = self.time_interval * self.reward_per_sec()\n",
    "        self.total_reward += r\n",
    "        \n",
    "        # Q learning\n",
    "        self.q_update(self.s, self.a, r, next_s)\n",
    "        self.s = next_s\n",
    "        self.a = next_a\n",
    "        \n",
    "        # output velocities\n",
    "        self.prev_nu, self.prev_omega = self.actions[next_a]\n",
    "        return self.actions[next_a]\n",
    "    \n",
    "    \n",
    "    def q_update(self, s, a, r, next_s):\n",
    "        if s == None: return\n",
    "        \n",
    "        q = self.statespace[s].q[a]\n",
    "        next_q = self.final_value if self.in_goal else self.statespace[next_s].max_q()\n",
    "        self.statespace[s].q[a] = (1 - self.alpha) * q + self.alpha * (r + next_q)\n",
    "        \n",
    "#         # log\n",
    "#         with open(\"log.txt\", \"a\") as f:\n",
    "#             f.write(\"{} {} {} prev_q:{:.2f}, next_step_max_q:{:.2f}, new_q:{:.2f}\\n\"\\\n",
    "#                     .format(s, r, next_s, q, next_q, self.statespace[s].q[a]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4662c5",
   "metadata": {},
   "source": [
    "## 11.1.6 recurse episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64699fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpRobot(Robot):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.init_agent = copy.deepcopy(self.agent)\n",
    "        \n",
    "    def choose_pose(self):\n",
    "        xy = random.random()*6 - 2 #x or y. around the center:4\n",
    "        t = random.random() * 2*math.pi\n",
    "        return np.array([3, xy, t].T) if random.random()>0.5 else np.array([xy, 3, t]).T\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset agent\n",
    "        tmp_ss = self.agent.statespace\n",
    "        self.agent = copy.deepcopy(self.init_agent)\n",
    "        self.agent.statespace = tmp_ss\n",
    "        \n",
    "        # set initial position\n",
    "        self.pose = self.choose_pose()\n",
    "        self.agent.estimator.belief = multivariate_normal(mean=self.pose, cov=np.diag([1e-10,1e-10,1e-10]))\n",
    "        \n",
    "        # delete black tracking lines\n",
    "        self.poses = []\n",
    "        \n",
    "    def one_step(self, time_interval):\n",
    "        if self.agent.update_end:\n",
    "            with open(\"log.txt\",\"a\") as f:\n",
    "                f.write(\"{}\\n\".format(self.agent.total_reward + self.agent.final_value))\n",
    "            self.reset()\n",
    "            return\n",
    "        super().one_step(time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(): \n",
    "    time_interval = 0.1\n",
    "    world = PuddleWorld(400000, time_interval, debug=False)  #長時間アニメーション時間をとる\n",
    "\n",
    "    ## 地図を生成して3つランドマークを追加 ##\n",
    "    m = Map()\n",
    "    for ln in [(-4,2), (2,-3), (4,4), (-4,-4)]: m.append_landmark(Landmark(*ln))\n",
    "    world.append(m)   \n",
    "\n",
    "    ##ゴールの追加##\n",
    "    goal = Goal(-3,-3) \n",
    "    world.append(goal)\n",
    "    \n",
    "    ##水たまりの追加##\n",
    "    world.append(Puddle((-2, 0), (0, 2), 0.1)) \n",
    "    world.append(Puddle((-0.5, -2), (2.5, 1), 0.1)) \n",
    "\n",
    "    ##ロボットを1台登場させる##\n",
    "    init_pose = np.array([3, 3, 0]).T\n",
    "    kf = KalmanFilter(m, init_pose)\n",
    "    a = QAgent(time_interval, kf)\n",
    "    r = WarpRobot(init_pose, sensor=Camera(m, distance_bias_rate_stddev=0, direction_bias_stddev=0), \n",
    "              agent=a, color=\"red\", bias_rate_stds=(0,0))\n",
    "    world.append(r)\n",
    "    \n",
    "    world.draw()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0405705",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a57133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probrobo",
   "language": "python",
   "name": "probrobo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
